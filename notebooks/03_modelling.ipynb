{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6db873b",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cce4ca",
   "metadata": {},
   "source": [
    "## Setup MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SETUP BLOCK (Run once per project lifetime)\n",
    "# import mlflow\n",
    "# import os\n",
    "\n",
    "# # 1. Define Absolute Paths\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# db_uri = f\"sqlite:///{os.path.join(project_root, 'mlflow.db')}\"\n",
    "# artifact_uri = \"file:///\" + os.path.join(project_root, \"mlruns\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "# # 2. Connect\n",
    "# mlflow.set_tracking_uri(db_uri)\n",
    "\n",
    "# # 3. Create (Safe Check)\n",
    "# if not mlflow.get_experiment_by_name(\"CreditGuard\"):\n",
    "#     mlflow.create_experiment(\"CreditGuard\", artifact_location=artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///../mlflow.db\")\n",
    "\n",
    "mlflow.set_experiment(\"CreditGuard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32b4a0",
   "metadata": {},
   "source": [
    "## EXP#2\n",
    "\n",
    "> EXP#1 is in `ebd3a2eaee0fc35a0ee7c14d3c5e12f5cf073b46` commit\n",
    "- handled missing value\n",
    "  - emp_length's na is replaced w/ 'Unknown' -> after FE imputed with mode + flagging\n",
    "  - mort_acc's na is imputed with median + flagging\n",
    "  - others with NA below ten is dropped\n",
    "- improved FE on encoding (added target encoding, made few adjustments) and converting to datetime (+issue_d)\n",
    "- classes is less imbalanced than EXP#1 (99/1 -> 80/20)\n",
    "- using tree-based models, such as: RF, XGB, LGBM\n",
    "- added feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/processed/train_fe.csv'\n",
    "TEST_DATA_PATH = '../data/processed/test_fe.csv'\n",
    "TARGET_COL = 'loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL])\n",
    "y_test = test_df[TARGET_COL]\n",
    "\n",
    "print(f\"Train shapes: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test shapes:  {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62eac08",
   "metadata": {},
   "source": [
    "### MLFlow Params Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "rf_params = {\n",
    "    \"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 4, \"class_weight\": \"balanced\",\n",
    "    \"random_state\": 42, \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 100, \"max_depth\": 6, \"learning_rate\": 0.1,\n",
    "    \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"scale_pos_weight\": 4,\n",
    "    \"random_state\": 42, \"n_jobs\": -1, \"eval_metric\": \"logloss\"\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    \"n_estimators\": 100, \"max_depth\": 7, \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 31, \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\": 4, \"random_state\": 42, \"n_jobs\": -1, \"verbose\": -1\n",
    "}\n",
    "\n",
    "# Data Processing (Executed via Scikit-Learn Pipeline in NB 02)\n",
    "data_params = {\n",
    "    \"pipeline_architecture\": \"ColumnTransformer\",\n",
    "    \"imputation_strategy\": \"Median (Num) / Mode (Cat) via Train Set Only\",\n",
    "    \"fe_encoding\": \"OrdinalEncoder + TargetEncoder + Dynamic OHE\",      \n",
    "    \"data_split_ratio\": \"80/20 (Stratified)\",\n",
    "    \"leakage_prevention\": \"Strict fit on X_train, transform on X_test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b00487",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93031171",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234901e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"EXP_02a_RF_Pipeline\"):\n",
    "    \n",
    "    # --- Log Hyperparameters & Data Preprocessing ---\n",
    "    mlflow.log_params(rf_params)\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    mlflow.log_param(\"input_rows\", X_train.shape[0])\n",
    "    mlflow.log_param(\"input_cols\", X_train.shape[1])\n",
    "    mlflow.log_param(\"column_names\", X_train.columns.tolist())\n",
    "\n",
    "    mlflow.log_artifact(\"01_eda.ipynb\", artifact_path=\"code_snapshot\")\n",
    "    mlflow.log_artifact(\"02_preprocessing.ipynb\", artifact_path=\"code_snapshot\")\n",
    "\n",
    "    # --- Train & Evaluate ---\n",
    "    print(\"Training RF Model...\")\n",
    "    rf_model = RandomForestClassifier(**rf_params, oob_score=True, warm_start=True)\n",
    "    \n",
    "    oob_scores = []    \n",
    "    test_scores = []\n",
    "    n_trees_range = range(10, rf_params['n_estimators'] + 1, 10)\n",
    "    \n",
    "    rf_model.n_estimators = 10\n",
    "    for n_trees in n_trees_range:\n",
    "        rf_model.n_estimators = n_trees\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        oob_scores.append(1 - rf_model.oob_score_)\n",
    "        y_pred_test = rf_model.predict(X_test)\n",
    "        test_error = 1 - accuracy_score(y_test, y_pred_test)\n",
    "        test_scores.append(test_error)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_prob = rf_model.predict_proba(X_test)[:, 1] \n",
    "\n",
    "    # --- Log Training Curves ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    epochs = list(n_trees_range)\n",
    "    \n",
    "    ax.plot(epochs, oob_scores, 'b-', labels='OOB Error', linewidth=2)\n",
    "    ax.plot(epochs, test_scores, 'b-', labels='Test Error', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Number of Trees')\n",
    "    ax.set_ylabel('Error Rate')\n",
    "    ax.set_title('Training & Validation Curves - Random Forest')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, 'training_curves_rf.png')\n",
    "    plt.close(fig)\n",
    "    print('Training curves logged.')\n",
    "\n",
    "    # --- Log Metrics ---\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    print(f\"Logged Metrics: {metrics}\")\n",
    "    \n",
    "    # --- Log Model ---\n",
    "    mlflow.sklearn.log_model(rf_model, name=\"model\")\n",
    "    \n",
    "    # --- Log Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_test, y_pred)    \n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    \n",
    "    labels = ['Fully Paid', 'Charged Off']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    \n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix - EXP_02a (RF)')\n",
    "    \n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: Permutation ---\n",
    "    print('Computing Permutation Importance...')\n",
    "    perm_importance = permutation_importance(\n",
    "        rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    perm_df = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': perm_importance.importances_mean,\n",
    "        'importance_std': perm_importance.importances_std\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_data = perm_df.head(15).sort_values('importance_mean', ascending=True)\n",
    "\n",
    "    ax.barh(\n",
    "        plot_data['feature'], plot_data['importance_mean'],\n",
    "        xerr=plot_data['importance_std'], capsize=3, color='steelblue'\n",
    "    )    \n",
    "    ax.set_xlabel('Importance (Mean Decrease in Accuracy)')\n",
    "    ax.set_title('Permutation Importance - RF')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, 'permutation_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: SHAP ---\n",
    "    print('Computing SHAP values...')\n",
    "    sample_size = min(500, len(X_test))\n",
    "    X_test_sample = X_test.iloc[:sample_size].reset_index(drop=True)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    # RF typically returns a list of arrays for binary classification\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "        \n",
    "    # Handle older shap versions if it returns a 3D array\n",
    "    if len(shap_vals.shape) == 3:\n",
    "        shap_vals = shap_vals[:, :, 1]  \n",
    "    \n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X_test_sample.columns,\n",
    "        'shap_importance': np.abs(shap_vals).mean(axis=0)\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_vals, X_test_sample, plot_type='bar', show=False)\n",
    "    plt.title('SHAP Importance - RF')\n",
    "    mlflow.log_figure(fig, 'shap_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    top_3_features = shap_importance.head(3)['feature'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, feature in enumerate(top_3_features):\n",
    "        feature_idx = X_test_sample.columns.get_loc(feature)\n",
    "        shap.dependence_plot(\n",
    "            feature_idx, shap_vals, X_test_sample, show=False, ax=axes[idx]\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, 'shap_dependence_plots.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Export Feature Importance --\n",
    "    print('Exporting feature importance data...')\n",
    "    \n",
    "    import os\n",
    "    os.makedirs('../importance', exist_ok=True)\n",
    "    PI_PATH = '../importance/permutation_importance_rf.csv'\n",
    "    SHAP_PATH = '../importance/shap_importance_rf.csv'\n",
    "    \n",
    "    perm_df.to_csv(PI_PATH, index=False)\n",
    "    mlflow.log_artifact(PI_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    shap_importance.to_csv(SHAP_PATH, index=False)\n",
    "    mlflow.log_artifact(SHAP_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    print(\"Run Complete. Notebooks, Params, Metrics, Models, and XAI Plots saved to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b30b3",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"EXP_02b_XGB_Pipeline\"):\n",
    "    \n",
    "    # --- Log Hyperparameters & Data Preprocessing ---\n",
    "    mlflow.log_params(xgb_params)\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    mlflow.log_param(\"input_rows\", X_train.shape[0])\n",
    "    mlflow.log_param(\"input_cols\", X_train.shape[1])\n",
    "    mlflow.log_param(\"column_names\", X_train.columns.tolist())\n",
    "\n",
    "    mlflow.log_artifact(\"01_eda.ipynb\", artifact_path=\"code_snapshot\")\n",
    "    mlflow.log_artifact(\"02_preprocessing.ipynb\", artifact_path=\"code_snapshot\")\n",
    "\n",
    "    # --- Train & Evaluate ---\n",
    "    print(\"Training XGB Model...\")\n",
    "    xgb_model = XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "    \n",
    "    results = xgb_model.evals_result()\n",
    "    train_loss = results['validation_0']['logloss']\n",
    "    test_loss = results['validation_1']['logloss']\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_prob = xgb_model.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "    # --- Log Training Curves ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    ax.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)\n",
    "    ax.plot(epochs, test_loss, 'r-', label='Test Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (Logloss)')\n",
    "    ax.set_title('Training & Validation Curves - XGBoost')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, \"training_curves_xgb.png\")\n",
    "    plt.close(fig)\n",
    "    print(\"Training curves logged.\")\n",
    "\n",
    "    # --- Log Metrics ---\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    print(f\"Logged Metrics: {metrics}\")\n",
    "    \n",
    "    # --- Log Model ---\n",
    "    mlflow.sklearn.log_model(xgb_model, name=\"model\")\n",
    "    \n",
    "    # --- Log Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_test, y_pred)    \n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    \n",
    "    labels = ['Fully Paid', 'Charged Off']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    \n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix - EXP_02b (XGB)')\n",
    "    \n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: Permutation ---\n",
    "    print('Computing Permutation Importance...')\n",
    "    perm_importance = permutation_importance(\n",
    "        xgb_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    perm_df = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': perm_importance.importances_mean,\n",
    "        'importance_std': perm_importance.importances_std\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_data = perm_df.head(15).sort_values('importance_mean', ascending=True)\n",
    "\n",
    "    ax.barh(\n",
    "        plot_data['feature'], plot_data['importance_mean'],\n",
    "        xerr=plot_data['importance_std'], capsize=3, color='steelblue'\n",
    "    )    \n",
    "    ax.set_xlabel('Importance (Mean Decrease in Accuracy)')\n",
    "    ax.set_title('Permutation Importance - XGB')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, 'permutation_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: SHAP ---\n",
    "    print('Computing SHAP values...')\n",
    "    sample_size = min(500, len(X_test))\n",
    "    X_test_sample = X_test.iloc[:sample_size].reset_index(drop=True)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X_test_sample.columns,\n",
    "        'shap_importance': np.abs(shap_vals).mean(axis=0)\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_vals, X_test_sample, plot_type='bar', show=False)\n",
    "    plt.title('SHAP Importance - XGB')\n",
    "    mlflow.log_figure(fig, 'shap_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    top_3_features = shap_importance.head(3)['feature'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, feature in enumerate(top_3_features):\n",
    "        feature_idx = X_test_sample.columns.get_loc(feature)\n",
    "        shap.dependence_plot(\n",
    "            feature_idx, shap_vals, X_test_sample, show=False, ax=axes[idx]\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, 'shap_dependence_plots.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Export Feature Importance --\n",
    "    print('Exporting feature importance data...')\n",
    "    \n",
    "    PI_PATH = '../importance/permutation_importance_xgb.csv'\n",
    "    SHAP_PATH = '../importance/shap_importance_xgb.csv'\n",
    "    \n",
    "    perm_df.to_csv(PI_PATH, index=False)\n",
    "    mlflow.log_artifact(PI_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    shap_importance.to_csv(SHAP_PATH, index=False)\n",
    "    mlflow.log_artifact(SHAP_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    print(\"Run Complete. Notebooks, Params, Metrics, Models, and XAI Plots saved to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03ce7d",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LightGBM\n",
    "with mlflow.start_run(run_name=\"EXP_02c_LGBM_Pipeline\"):\n",
    "    \n",
    "    # --- Log Hyperparameters & Data Preprocessing ---\n",
    "    mlflow.log_params(lgbm_params)\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    mlflow.log_param(\"input_rows\", X_train.shape[0])\n",
    "    mlflow.log_param(\"input_cols\", X_train.shape[1])\n",
    "    mlflow.log_param(\"column_names\", X_train.columns.tolist())\n",
    "\n",
    "    mlflow.log_artifact(\"01_eda.ipynb\", artifact_path=\"code_snapshot\")\n",
    "    mlflow.log_artifact(\"02_preprocessing.ipynb\", artifact_path=\"code_snapshot\")\n",
    "\n",
    "    # --- Train & Evaluate ---\n",
    "    print(\"Training LGBM Model...\")\n",
    "    lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "    lgbm_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "    \n",
    "    train_loss = lgbm_model.evals_result_['valid_0']['binary_logloss']\n",
    "    test_loss = lgbm_model.evals_result_['valid_1']['binary_logloss']\n",
    "    \n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_prob = lgbm_model.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "    # --- Log Training Curves ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    ax.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)\n",
    "    ax.plot(epochs, test_loss, 'r-', label='Test Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (Logloss)')\n",
    "    ax.set_title('Training & Validation Curves - LightGBM')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, \"training_curves_lgbm.png\")\n",
    "    plt.close(fig)\n",
    "    print(\"Training curves logged.\")\n",
    "\n",
    "    # --- Log Metrics ---\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    print(f\"Logged Metrics: {metrics}\")\n",
    "    \n",
    "    # --- Log Model ---\n",
    "    mlflow.sklearn.log_model(lgbm_model, name=\"model\")\n",
    "    \n",
    "    # --- Log Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_test, y_pred)    \n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    \n",
    "    labels = ['Fully Paid', 'Charged Off']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    \n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix - EXP_02c (LGBM)')\n",
    "    \n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: Permutation ---\n",
    "    print('Computing Permutation Importance...')\n",
    "    perm_importance = permutation_importance(\n",
    "        lgbm_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    perm_df = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': perm_importance.importances_mean,\n",
    "        'importance_std': perm_importance.importances_std\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_data = perm_df.head(15).sort_values('importance_mean', ascending=True)\n",
    "\n",
    "    ax.barh(\n",
    "        plot_data['feature'], plot_data['importance_mean'],\n",
    "        xerr=plot_data['importance_std'], capsize=3, color='steelblue'\n",
    "    )    \n",
    "    ax.set_xlabel('Importance (Mean Decrease in Accuracy)')\n",
    "    ax.set_title('Permutation Importance - LGBM')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    mlflow.log_figure(fig, 'permutation_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance: SHAP ---\n",
    "    print('Computing SHAP values...')\n",
    "    sample_size = min(500, len(X_test))\n",
    "    X_test_sample = X_test.iloc[:sample_size].reset_index(drop=True)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X_test_sample.columns,\n",
    "        'shap_importance': np.abs(shap_vals).mean(axis=0)\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_vals, X_test_sample, plot_type='bar', show=False)\n",
    "    plt.title('SHAP Importance - LGBM')\n",
    "    mlflow.log_figure(fig, 'shap_importance.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    top_3_features = shap_importance.head(3)['feature'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, feature in enumerate(top_3_features):\n",
    "        feature_idx = X_test_sample.columns.get_loc(feature)\n",
    "        shap.dependence_plot(\n",
    "            feature_idx, shap_vals, X_test_sample, show=False, ax=axes[idx]\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, 'shap_dependence_plots.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # --- Export Feature Importance --\n",
    "    print('Exporting feature importance data...')\n",
    "    \n",
    "    PI_PATH = '../importance/permutation_importance_lgbm.csv'\n",
    "    SHAP_PATH = '../importance/shap_importance_lgbm.csv'\n",
    "    \n",
    "    perm_df.to_csv(PI_PATH, index=False)\n",
    "    mlflow.log_artifact(PI_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    shap_importance.to_csv(SHAP_PATH, index=False)\n",
    "    mlflow.log_artifact(SHAP_PATH, artifact_path='feature_importance')\n",
    "    \n",
    "    print(\"Run Complete. Notebooks, Params, Metrics, Models, and XAI Plots saved to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888e433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
