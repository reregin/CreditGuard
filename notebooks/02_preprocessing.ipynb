{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54bbacb",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499698b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, TargetEncoder\n",
    "\n",
    "CLEANED_DATA_PATH = '../data/processed/lending-club-cleaned.csv'\n",
    "# Save separate train and test files to maintain the vault!\n",
    "TRAIN_DATA_PATH = '../data/processed/train_fe.csv'\n",
    "TEST_DATA_PATH = '../data/processed/test_fe.csv'\n",
    "PIPELINE_PATH = '../models/preprocessor.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24567403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaf01b",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "df_prep = df.copy()\n",
    "\n",
    "df_prep['loan_status'] = df_prep['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "X = df_prep.drop(columns=['loan_status'])\n",
    "y = df_prep['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a347c5",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn >= 1.2 allows us to output clean Pandas DataFrames instead of messy Numpy Arrays\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Blueprint 1: emp_length (Needs BOTH Mode Imputation AND Ordinal Encoding)\n",
    "emp_categories = ['< 1 year', '1 year', '2 years', '3 years', '4 years', \n",
    "                  '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']\n",
    "\n",
    "emp_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[emp_categories]))\n",
    "])\n",
    "\n",
    "# Blueprint 2: Standard Ordinals (No missing values expected here)\n",
    "ord_cols = ['term', 'grade', 'sub_grade', 'verification_status']\n",
    "ord_categories = [\n",
    "    [' 36 months', ' 60 months'],\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G'],\n",
    "    ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5',\n",
    "     'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5',\n",
    "     'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5',\n",
    "     'G1', 'G2', 'G3', 'G4', 'G5'],\n",
    "    ['Not Verified', 'Verified', 'Source Verified']\n",
    "]\n",
    "\n",
    "ord_pipe = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(categories=ord_categories))\n",
    "])\n",
    "\n",
    "# Blueprint 3: mort_acc (Needs Median Imputation)\n",
    "mort_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95582c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_categorical_cols = ['emp_length', 'term', 'grade', 'sub_grade', 'verification_status', 'purpose']\n",
    "\n",
    "all_categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "ohe_cols = [col for col in all_categorical_cols if col not in handled_categorical_cols]\n",
    "\n",
    "print(f\"Dynamically detected columns for OHE: {ohe_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('emp', emp_pipe, ['emp_length']),\n",
    "    ('ord', ord_pipe, ord_cols),\n",
    "    ('mort', mort_pipe, ['mort_acc']),\n",
    "    ('target', TargetEncoder(target_type='binary'), ['purpose']),\n",
    "    \n",
    "    ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), ohe_cols)\n",
    "    \n",
    "], remainder='passthrough') \n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "preprocessor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0198cf",
   "metadata": {},
   "source": [
    "### Apply Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the learned rules to both sets securely\n",
    "X_train_fe = preprocessor.transform(X_train)\n",
    "X_test_fe = preprocessor.transform(X_test)\n",
    "\n",
    "# Optional: Scikit-learn adds prefixes like 'remainder__' to column names. \n",
    "# This quick loop cleans them up so your columns look normal again.\n",
    "X_train_fe.columns = [col.split('__')[-1] for col in X_train_fe.columns]\n",
    "X_test_fe.columns = [col.split('__')[-1] for col in X_test_fe.columns]\n",
    "\n",
    "print(\"Transformations applied.\")\n",
    "print(f\"Final Training Features Shape: {X_train_fe.shape}\")\n",
    "print(f\"Final Testing Features Shape:  {X_test_fe.shape}\")\n",
    "\n",
    "# Verify no nulls remain in the features\n",
    "print(f\"\\nRemaining Nulls in Train:\\n{X_train_fe.isnull().sum()[X_train_fe.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6214bf5",
   "metadata": {},
   "source": [
    "### Save Dataset & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4073c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Reattach the target variable temporarily just to save clean CSVs for Notebook 03\n",
    "train_df = pd.concat([X_train_fe, y_train.reset_index(drop=True)], axis=1)\n",
    "test_df = pd.concat([X_test_fe, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "# Save the actual fitted pipeline! \n",
    "joblib.dump(preprocessor, PIPELINE_PATH)\n",
    "\n",
    "print(f\"Saved Train Data to: {TRAIN_DATA_PATH}\")\n",
    "print(f\"Saved Test Data to:  {TEST_DATA_PATH}\")\n",
    "print(f\"Saved Preprocessor Pipeline to: {PIPELINE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
